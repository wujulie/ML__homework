# -*- coding: utf-8 -*-
"""homework_4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y6ENUqyo5uzkRrTRw5K1Ns77c2cEps_u
"""

import numpy as np
import pandas as pd
from sklearn.svm import SVC
data=pd.read_csv("/content/drive/MyDrive/bank.csv",delimiter=";",header='infer')
pd.set_option('display.max_columns',None) 
data.shape

from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()

data.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),(1,2,3,4,5,6,7,8,9,10,11,12),inplace=True)

data.head(3)

data.dtypes

job=data['job'].unique()
marital = data['marital'].unique()
education = data['education'].unique()
default = data['default'].unique()
housing = data['housing'].unique()
loan = data['loan'].unique()
contact = data['contact'].unique()
poutcome = data['poutcome'].unique()
y = data['y'].unique()
y

data['job'] = LE.fit_transform(data['job'])
data['marital'] = LE.fit_transform(data['marital'])
data['education'] = LE.fit_transform(data['education'])
data['default'] = LE.fit_transform(data['default'])
data['housing'] = LE.fit_transform(data['housing'])
data['loan'] = LE.fit_transform(data['loan'])
data['contact']= LE.fit_transform(data['contact'])
data['poutcome'] = LE.fit_transform(data['poutcome'])
data['y']= LE.fit_transform(data['y'])

data.head(3)

data.dtypes

data.isnull().sum()

X=data.iloc[:,:-1]#X
Y=data.iloc[:,-1]#Y

X.head()

from sklearn.preprocessing import StandardScaler
X_std = StandardScaler().fit_transform(X)
print(X_std.var(axis=1))
from sklearn.decomposition import PCA
pca = PCA(n_components=10)
x_pca = pca.fit_transform(X_std)

x_pca.shape

from sklearn.model_selection import train_test_split,cross_val_score
X_train, X_test, Y_train, Y_test = train_test_split(x_pca, Y,test_size=0.2,random_state=1000)

X_train.shape

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
svc = SVC(C=1,kernel='linear')
print('SVM score:',cross_val_score(svc, X_train, Y_train, scoring='accuracy',cv=10).mean())



from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model=Sequential()
model.add(Dense(50, activation='sigmoid', input_shape=(10,)))#加入第一層hidden Layer神經元數(維度)為50，inputshape為10，激勵函數為sigmoid
model.add(Dense(30, activation='sigmoid'))#加入第二層hidden Layer神經元數(維度)為30，inputshape為50，激勵函數為sigmoid
model.add(Dense(1, activation='sigmoid'))#加入Output Layer 神經元數(維度)為1，inputshape為30，，激勵函數為sigmoid
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])#優化器為adam,loss為binary_crossentropy,metrics為accuracy
history = model.fit(X_train, Y_train, epochs=100, batch_size=20)#開始訓練,epoch為100,batch_size為20
model.summary()#輸出模型架構
test_los,test_acc=model.evaluate(X_test, Y_test)#準確率

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
model_1=Sequential()
model_1.add(Dense(1, activation='sigmoid'))
model_1.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])
history_1 = model_1.fit(X_train, Y_train, epochs=100,batch_size=20)

model_1.summary()

model_1.evaluate(X_test, Y_test,verbose=0)

import matplotlib.pyplot as plt
plt.title('MLNN_loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.plot(history.history['loss'])

import matplotlib.pyplot as plt
plt.title('MLNN_accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.plot(history.history['accuracy'])

import matplotlib.pyplot as plt
plt.title('logistic regression_loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.plot(history_1.history['loss'])

import matplotlib.pyplot as plt
plt.title('logistic regression_accuracy')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.plot(history_1.history['accuracy'])